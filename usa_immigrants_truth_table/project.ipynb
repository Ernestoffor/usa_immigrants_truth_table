{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truth Table of USA immigrants data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import count, col, isnan, when, udf\n",
    "from pyspark.sql.types import StructType as ST, StructField as SF, DoubleType as Db, IntegerType as INT,\\\n",
    "StringType as S, DateType as DT, TimestampType as T, LongType as L\n",
    "import psycopg2\n",
    "import os\n",
    "import configparser\n",
    "from sql_queries import copy_table_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define a schema for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoSchema = ST([\n",
    "    SF(\"city\", S()),\n",
    "    SF(\"state\", S()),\n",
    "    SF(\"median_Age\", Db()),\n",
    "    SF(\"male_population\", INT()),\n",
    "    SF(\"female_population\", INT()),\n",
    "    SF(\"total_population\", INT()),\n",
    "    SF(\"number_of_veterans\", INT()),\n",
    "    SF(\"foreign_born\", INT()),\n",
    "    SF(\"average_household_size\", Db()),\n",
    "    SF(\"state_code\", S()),\n",
    "    SF(\"race\", S()),\n",
    "    SF(\"count\", INT())\n",
    "])\n",
    "\n",
    "modeSchema = ST([\n",
    "    SF(\"mode_code\", INT()),\n",
    "    SF(\"mode_name\", S())\n",
    "    \n",
    "])\n",
    "\n",
    "portSchema = ST([\n",
    "    SF(\"port_code\", S()),\n",
    "    SF(\"port_name\", S()),\n",
    "    SF(\"state_code\", S())\n",
    "])\n",
    "\n",
    "visaSchema = ST([\n",
    "    SF(\"visa_code\", INT()),\n",
    "    SF(\"visatype\", S()),\n",
    "    SF(\"category\", S()),\n",
    "    SF(\"description\", S())\n",
    "])\n",
    "\n",
    "countrySchema = ST([\n",
    "   SF(\"country_code\", INT()),\n",
    "   SF(\"country\", S())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data from different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df =spark.read.load('./sas_data')\n",
    "\n",
    "df_demograph= spark.read .format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"delimiter\",\";\")\\\n",
    ".schema(demoSchema).load(\"us-cities-demographics.csv\")\n",
    "\n",
    "port_df= spark.read .format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".schema(portSchema).load(\"port.csv\")\n",
    "\n",
    "visatype_df= spark.read .format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".schema(visaSchema).load(\"visatype.csv\")\n",
    "\n",
    "\n",
    "mode_of_arrival_df= spark.read .format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".schema(modeSchema).load(\"mode_of_arrival.csv\")\n",
    "\n",
    "country_df= spark.read .format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".schema(countrySchema).load(\"countries_and_cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNullFunc(df):\n",
    "    \"\"\"\n",
    "    A procedure to clean up the immigration dataframe by filling in the null values of needed columns as follows:\n",
    "    occup - fill null values with \"unknown\" (the column has 3088187 null values)\n",
    "    dtadfile - fill the only null value with \"20210618\" date of this implementation\n",
    "    i94bir - fill the null values with \"-1\" - age cannot be negative (there are 802 null values in the column)\n",
    "    depdate - fill the null values with \"23146\"  a future date of this implementation (there are 142457) missing values here)\n",
    "    i94addr - fill missing values with \"unknown\" (the column has 152592 missing values)\n",
    "    i94mode - fill missing values with \"4\" corresponding to unknown in the visatype table (the column has 239 null values)\n",
    "    biryear - fill missing values with \"2021\" (there are 802 missing values here)\n",
    "    gender - fill missing values with \"unknown\" (there are 44269 null values in the column)\n",
    "    INPUT ARG:-> dataframe to be cleaned (immigration_df)\n",
    "    RETURNS:-> Cleaned Dataframe\n",
    "    \"\"\"\n",
    "    df= df.fillna(value= 23146 , subset = [\"depdate\"])\n",
    "    df= df.fillna(value= \"20210618\" , subset = [\"dtadfile\"])\n",
    "    df= df.fillna(value= -1 , subset = [\"i94bir\"])\n",
    "    df= df.fillna(value= \"unkown\" , subset = [\"i94addr\", \"occup\", \"gender\"])\n",
    "    df= df.fillna(value= 4 , subset = [\"i94mode\"])\n",
    "    df= df.fillna(value= 2021 , subset = [\"biryear\"])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the \"arrdate\" and \"depdate\" to timestamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = datetime(1960, 1, 1)\n",
    "date_converter = udf(lambda x : epoch + timedelta(days = int(x)), DT())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def castColumnFunc(df):\n",
    "    \"\"\"\n",
    "    A function to cast the following columns to the appropriate data types and create respective new columns for them\n",
    "    INPUT ARG: Dataframe to be transformed\n",
    "    RETURNS :-> a transformed dataframe\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"arr_date\", date_converter(immigration_df[\"arrdate\"]))\n",
    "    df = df.withColumn(\"dep_date\", date_converter(immigration_df[\"depdate\"]))\n",
    "    df = df.withColumn(\"age\", immigration_df[\"i94bir\"].cast(INT()))\n",
    "    df = df.withColumn(\"year_of_birth\", immigration_df[\"biryear\"].cast(INT()))\n",
    "    df = df.withColumn(\"city_code\", immigration_df[\"i94cit\"].cast(INT()))\n",
    "    df = df.withColumn(\"country_code\", immigration_df[\"i94res\"].cast(INT()))\n",
    "    df = df.withColumn(\"month\", immigration_df[\"i94mon\"].cast(INT()))\n",
    "    df = df.withColumn(\"id\", immigration_df[\"cicid\"].cast(INT()))\n",
    "    df = df.withColumn(\"visa\", immigration_df[\"i94visa\"].cast(INT()))\n",
    "    df = df.withColumn(\"year_of_data\", immigration_df[\"i94yr\"].cast(INT()))\n",
    "    df = df.withColumn(\"mode_code\", immigration_df[\"i94mode\"].cast(INT()))\n",
    "    df = df.withColumn(\"date_added_to_file\", immigration_df[\"dtadfile\"].cast(DT()))\n",
    "    df = df.withColumn(\"port_code\", immigration_df[\"i94port\"].cast(S()))\n",
    "    df = df.withColumn(\"state_code\", immigration_df[\"i94addr\"].cast(S()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDimFactTables(immigration_df, mode_of_arrival_df, port_df, visatype_df, country_df, df_demogragh):\n",
    "    \"\"\"\n",
    "    A function: -> to extract dimensional and fact tables as follows:\n",
    "    immigration_fact, date_dim and immigrant_dim tables from immigration_df\n",
    "    demograph_dim table from df_demograph\n",
    "    port_dim table from port_df\n",
    "    visa_dim table from visatype_df\n",
    "    mode_of_arrival_dim table from mode_of_arrival_df\n",
    "    country_dim table from country_df\n",
    "    INPUTS: -> Dataframes(immigration_df, mode_of_arrival_df, port_df, visatype_df, country_df, df_demogragh) to extract from\n",
    "    RETURNS: -> fact and dimensional tables\n",
    "    \"\"\"\n",
    "    immigration_fact = immigration_df.select([\"id\", \"arr_date\", \"country_code\", \"state_code\", \"port_code\", \"mode_code\", \"visatype\"])\n",
    "    date_dim = immigration_df.select([\"arr_date\", \"dep_date\", \"month\", \"year_of_data\"])\n",
    "    immigrant_dim = immigration_df.select([\"id\", \"age\", \"year_of_birth\", \"gender\"])\n",
    "    mode_of_arrival_dim = mode_of_arrival_df.select([\"mode_code\",\"mode_name\"])\n",
    "    port_dim = port_df.select([\"port_code\", \"port_name\", \"state_code\"])\n",
    "    visa_dim = visatype_df.select([\"visa_code\", \"visatype\", \"category\", \"description\"])\n",
    "    country_dim = country_df.select([\"country_code\", \"country\"])\n",
    "    demograph_dim = df_demograph.select([\"city\", \"state\", \"state_code\", \"median_age\", \"male_population\", \"female_population\",\n",
    "                                    \"total_population\", \"number_of_veterans\", \"race\", \"average_household_size\", \"count\"])\n",
    "    return (immigration_fact, date_dim, immigrant_dim, mode_of_arrival_dim, port_dim, visa_dim, country_dim, demograph_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_func(output, list, array_of_paths):\n",
    "    \"\"\"\n",
    "    Procedure : to write or save tables\n",
    "    INPUT:\n",
    "        * output - path to save the table\n",
    "        * list - array of tables\n",
    "        * array_of_paths - list of unique paths\n",
    "    RETURNS : NONE\n",
    "    \"\"\"\n",
    "    for table, pathway in zip(list,array_of_paths):\n",
    "        #table.write.mode(\"overwrite\").csv(os.path.join(output,pathway ))\n",
    "        file_path = os.path.join(output,pathway )\n",
    "        table.write.mode(\"overwrite\").csv(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    A function:-> to load the tables from AWS S3 into the redshift\n",
    "    INPUT ARGS: \n",
    "        cur:-> cursor to execute database operation\n",
    "        conn:-> connection to progres database via psycopg2 driver\n",
    "    RETURNS:-> None\n",
    "    \"\"\"\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_func_in_files(arrayOfTables):\n",
    "    \"\"\"\n",
    "    A function to write or save the extracted tables or dataframes into different files in CSV FORMAT.\n",
    "    INPUT/ARGUMENT: -> a list of tables specifically, [immigration_df, mode_of_arrival_df, port_df, visatype_df, \\\n",
    "    country_df, df_demograph]\n",
    "    RETURNS: -> None\n",
    "    \"\"\"\n",
    "    fact, dim1, dim2, dim3, dim4, dim5, dim6, dim7 = extractDimFactTables(*arrayOfTables)\n",
    "    fact.toPandas().to_csv('immigration_fact.csv')\n",
    "    dim1.toPandas().to_csv('date_dim.csv')\n",
    "    dim2.toPandas().to_csv('immigrant_dim.csv')\n",
    "    dim3.toPandas().to_csv('mode_of_arrival_dim.csv')\n",
    "    dim4.toPandas().to_csv('port_dim.csv')\n",
    "    dim5.toPandas().to_csv('visa_dim.csv')\n",
    "    dim6.toPandas().to_csv('country_dim.csv')\n",
    "    dim7.toPandas().to_csv('demograph_dim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualityCheckFunc(arrayOfTables, table_names_list):\n",
    "    \"\"\"\n",
    "    A procedure to check the completeness of all the tables\n",
    "    INPUTS:-\n",
    "        * arrayOfTables -> A list of the fact and dimensional tables\n",
    "        * table_names_list -> a list of the names of all the fact and dimensional tables\n",
    "    RETURNS:\n",
    "        :-> None\n",
    "    \"\"\"\n",
    "    tables = extractDimFactTables(*arrayOfTables)\n",
    "    for table, name in zip(tables, table_names_list):\n",
    "        if table.count() <=0:\n",
    "            print(\"The {} table is not complete\".format(name))\n",
    "        else:\n",
    "            print(\"The {} table has {} number of rows\".format(name, table.count()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The immigration_fact table has 3096313 number of rows\n",
      "The immigrant_dim table has 3096313 number of rows\n",
      "The date_dim table has 3096313 number of rows\n",
      "The mode_of_arrival_dim table has 5 number of rows\n",
      "The port_dim table has 660 number of rows\n",
      "The visa_dim table has 20 number of rows\n",
      "The country_dim table has 289 number of rows\n",
      "The demograph_dim table has 2891 number of rows\n"
     ]
    }
   ],
   "source": [
    "immigration_df = fillNullFunc(immigration_df)\n",
    "immigration_df = castColumnFunc(immigration_df)\n",
    "arrayOfTables = [immigration_df, mode_of_arrival_df, port_df, visatype_df, country_df, df_demograph]\n",
    "immigration_fact, date_dim, immigrant_dim, mode_of_arrival_dim, port_dim, visa_dim, country_dim, demograph_dim\\\n",
    "        = extractDimFactTables(*arrayOfTables)\n",
    "list2 = [\"immigration_fact\", \"immigrant_dim\", \"date_dim\", \"mode_of_arrival_dim\", \"port_dim\", \"visa_dim\", \"country_dim\", \"demograph_dim\"]\n",
    "\n",
    "write_func_in_files(arrayOfTables)\n",
    "qualityCheckFunc(arrayOfTables, list2)\n",
    "#output = \"s3a://data-polake-offorernest/project/\"\n",
    "output = \"output/\"\n",
    "\n",
    "write_func(output, arrayOfTables, list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "    \n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    load_staging_tables(cur, conn)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
